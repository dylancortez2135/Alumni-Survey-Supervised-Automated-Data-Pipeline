{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f11af6-d11f-4aaa-b906-6ab47582bfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema Validator\n",
    "\n",
    "import boto3\n",
    "import csv\n",
    "import io\n",
    "import os\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "EXPECTED_SCHEMA = {\n",
    "    \"Employee_ID\": \"int\",\n",
    "    \"Employee_Name\": \"str\",\n",
    "    \"College_Degree\": \"str\",\n",
    "    \"Department\": \"str\",\n",
    "    \"Job_Role\": \"str\",\n",
    "    \"DMC_Campus\": \"str\",\n",
    "    \"Email\": \"str\",\n",
    "    \"Phone_Number\": \"str\",\n",
    "    \"Performance_Rating\": \"int\"\n",
    "} # <-- Configure according to your expected schema\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    for record in event[\"Records\"]:\n",
    "        bucket = record[\"s3\"][\"bucket\"][\"name\"]\n",
    "        key = record[\"s3\"][\"object\"][\"key\"]\n",
    "\n",
    "        if not key.startswith(\"raw/\"):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "            body = obj[\"Body\"].read().decode(\"utf-8\").strip()\n",
    "            reader = csv.DictReader(io.StringIO(body))\n",
    "            headers = reader.fieldnames\n",
    "\n",
    "            if not headers or headers != list(EXPECTED_SCHEMA.keys()):\n",
    "                move_file(bucket, key, \"invalid/\")\n",
    "                print(f\"{key}: ❌ Invalid schema.\")\n",
    "                continue\n",
    "\n",
    "            bad_row = False\n",
    "            for row in reader:\n",
    "                for col, dtype in EXPECTED_SCHEMA.items():\n",
    "                    value = row.get(col, \"\").strip()\n",
    "                    if value and dtype == \"int\" and not value.isdigit():\n",
    "                        bad_row = True\n",
    "                        break\n",
    "                if bad_row:\n",
    "                    break\n",
    "\n",
    "            dest = \"invalid/\" if bad_row else \"valid/\"\n",
    "            move_file(bucket, key, dest)\n",
    "            print(f\"{key}: {'❌ Invalid data types.' if bad_row else '✅ Validation passed.'}\")\n",
    "\n",
    "        except Exception:\n",
    "            move_file(bucket, key, \"invalid/\")\n",
    "            print(f\"{key}: ⚠️ Validation failed due to an error.\")\n",
    "\n",
    "def move_file(bucket, source_key, dest_prefix):\n",
    "    filename = os.path.basename(source_key)\n",
    "    dest_key = f\"{dest_prefix}{filename}\"\n",
    "    s3.copy_object(Bucket=bucket, CopySource=f\"{bucket}/{source_key}\", Key=dest_key)\n",
    "    s3.delete_object(Bucket=bucket, Key=source_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc66239-d77f-4ad5-8864-c1d8aa58daaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Transformation Logic\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r\"YOUR_FILE\")\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "df['Employee_Name'] = df['Employee_Name'].str.title().str.strip()\n",
    "df['Employee_Name'] = df['Employee_Name'].str.replace(r'^(Dr\\.|Mr\\.|Ms\\.|Mrs\\.)\\s*|,?\\s*(DVM|MD|PhD|DDS|Esq\\.|Jr\\.|Sr\\.)$', '', regex=True).str.strip()\n",
    "\n",
    "df['Job_Role'] = df['Job_Role'].str.title().str.strip()\n",
    "\n",
    "df['Email'] = df['Email'].str.replace('(?i)example', 'gmail', regex=True)\n",
    "df['Email'] = df['Email'].str.replace(r'(@)(.*)', lambda m: m.group(1) + m.group(2).lower(), regex=True)\n",
    "\n",
    "df['Phone_Number'] = '+63 ' + df['Phone_Number'].astype(str)\n",
    "df\n",
    "\n",
    "# Converted to PySpark\n",
    "\n",
    "import sys\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "from pyspark.context import SparkContext\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "# Read valid data\n",
    "dyf = glueContext.create_dynamic_frame.from_options(\n",
    "    connection_type=\"s3\",\n",
    "    connection_options={\"paths\": [\"s3://YOUR_BUCKET/valid/\"]},\n",
    "    format=\"csv\",\n",
    "    format_options={\"withHeader\": True}\n",
    ")\n",
    "df = dyf.toDF().dropDuplicates()\n",
    "\n",
    "# Clean columns\n",
    "df = df.withColumn(\n",
    "    \"Employee_Name\",\n",
    "    F.regexp_replace(\n",
    "        F.initcap(F.trim(F.col(\"Employee_Name\"))),\n",
    "        r'^(Dr\\.|Mr\\.|Ms\\.|Mrs\\.)\\s*|,?\\s*(DVM|MD|PhD|DDS|Esq\\.|Jr\\.|Sr\\.)$',\n",
    "        ''\n",
    "    )\n",
    ").withColumn(\n",
    "    \"Job_Role\", F.initcap(F.trim(F.col(\"Job_Role\")))\n",
    ").withColumn(\n",
    "    \"Email\", F.regexp_replace(F.col(\"Email\"), '(?i)example', 'gmail')\n",
    ").withColumn(\n",
    "    \"Email\",\n",
    "    F.concat(\n",
    "        F.regexp_extract(F.col(\"Email\"), r'^[^@]+', 0),\n",
    "        F.lit('@'),\n",
    "        F.lower(F.regexp_extract(F.col(\"Email\"), r'@(.+)', 1))\n",
    "    )\n",
    ").withColumn(\n",
    "    \"Phone_Number\", F.concat(F.lit(\"+63 \"), F.col(\"Phone_Number\").cast(\"string\"))\n",
    ").filter(\n",
    "    F.col(\"Email\").isNotNull() & F.col(\"Phone_Number\").isNotNull()\n",
    ")\n",
    "\n",
    "# Write cleaned data\n",
    "cleaned_dyf = DynamicFrame.fromDF(df, glueContext, \"cleaned_dyf\")\n",
    "sink = glueContext.getSink(\n",
    "    path=\"s3://YOUR_BUCKET/transformed/\",\n",
    "    connection_type=\"s3\",\n",
    "    updateBehavior=\"UPDATE_IN_DATABASE\",\n",
    "    compression=\"snappy\",\n",
    "    enableUpdateCatalog=True,\n",
    "    transformation_ctx=\"sink\"\n",
    ")\n",
    "sink.setCatalogInfo(catalogDatabase=\"YOUR_DATABASE_NAME\", catalogTableName=\"YOUR_CLEAN_TABLE_NAME\")\n",
    "sink.setFormat(\"glueparquet\")\n",
    "sink.writeFrame(cleaned_dyf)\n",
    "\n",
    "# Archive\n",
    "s3 = boto3.client(\"s3\")\n",
    "bucket = \"YOUR_BUCKET_NAME\"\n",
    "response = s3.list_objects_v2(Bucket=bucket, Prefix=\"transformed/\")\n",
    "today = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "archive_prefix = f\"archive/{today}/\"\n",
    "\n",
    "if \"Contents\" in response:\n",
    "    for obj in response[\"Contents\"]:\n",
    "        copy_source = {\"Bucket\": bucket, \"Key\": obj[\"Key\"]}\n",
    "        archive_key = obj[\"Key\"].replace(\"transformed/\", archive_prefix)\n",
    "        s3.copy_object(\n",
    "            CopySource=copy_source,\n",
    "            Bucket=bucket,\n",
    "            Key=archive_key,\n",
    "            StorageClass=\"DEEP_ARCHIVE\"\n",
    "        )\n",
    "\n",
    "print(\"✅ Glue transformation and archiving complete.\")\n",
    "job.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd68915c-4209-4cf4-a477-0b2f9b33db40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAMBDA-ATHENA GLOBAL DEDUPLICATION\n",
    "\n",
    "import boto3\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    athena = boto3.client('athena')\n",
    "\n",
    "    query = \"\"\"\n",
    "    CREATE OR REPLACE VIEW \"YOUR_DATABASE_NAME\".\"YOUR_DEDUPLICATED_NAME\" AS\n",
    "    SELECT *\n",
    "    FROM (\n",
    "        SELECT *,\n",
    "               ROW_NUMBER() OVER (PARTITION BY employee_id) AS rn\n",
    "        FROM \"YOUR_DATABASE_NAME\".\"YOUR_CLEAN_TABLE_NAME\"\n",
    "    )\n",
    "    WHERE rn = 1;\n",
    "    \"\"\"\n",
    "\n",
    "    response = athena.start_query_execution(\n",
    "        QueryString=query,\n",
    "        QueryExecutionContext={\n",
    "            'Catalog': 'AwsDataCatalog',\n",
    "            'Database': 'YOUR_DATABASE_NAME'\n",
    "        },\n",
    "        ResultConfiguration={\n",
    "            'OutputLocation': 's3://YOUR_ATHENA_QUERY_LOCATION/'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(f\"Athena deduplication view created. Query ID: {response['QueryExecutionId']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6d2925-6c42-449d-80f3-9b153697f89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python File Exporter. You can automate this using a method of your own (e.g., Python schedule Library, Windows Task Scheduler, and etc.)\n",
    "\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import io\n",
    "from datetime import datetime\n",
    "\n",
    "ACCESS_KEY = \"YOUR_ACCESS_KEY\"\n",
    "SECRET_KEY = \"YOUR_SECRET_KEY\"\n",
    "REGION = \"YOUR_REGION\"\n",
    "BUCKET_NAME = \"YOUR_BUCKET_NAME\"\n",
    "\n",
    "sheet_url = \"YOUR_DOCS_LINK/export?format=csv\"\n",
    "\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    aws_access_key_id=ACCESS_KEY,\n",
    "    aws_secret_access_key=SECRET_KEY,\n",
    "    region_name=REGION\n",
    ")\n",
    "\n",
    "df = pd.read_csv(sheet_url)\n",
    "\n",
    "if \"Submission ID\" in df.columns:\n",
    "    df = df.drop(columns=[\"Submission ID\"])\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "file_name = f\"google_sheet_responses_{timestamp}.csv\"\n",
    "s3_key = f\"raw/{file_name}\"\n",
    "\n",
    "csv_buffer = io.StringIO()\n",
    "df.to_csv(csv_buffer, index=False)\n",
    "\n",
    "s3.put_object(Bucket=BUCKET_NAME, Key=s3_key, Body=csv_buffer.getvalue())\n",
    "\n",
    "print(f\"✅ Successfully uploaded full sheet to s3://{BUCKET_NAME}/{s3_key}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
