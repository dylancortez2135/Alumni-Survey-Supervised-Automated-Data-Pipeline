{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f11af6-d11f-4aaa-b906-6ab47582bfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema Validator\n",
    "\n",
    "import boto3\n",
    "import csv\n",
    "import io\n",
    "import os\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "EXPECTED_SCHEMA = {\n",
    "    \"Employee_ID\": \"int\",\n",
    "    \"Employee_Name\": \"str\",\n",
    "    \"College_Degree\": \"str\",\n",
    "    \"Department\": \"str\",\n",
    "    \"Job_Role\": \"str\",\n",
    "    \"DMC_Campus\": \"str\",\n",
    "    \"Email\": \"str\",\n",
    "    \"Phone_Number\": \"int\",\n",
    "    \"Performance_Rating\": \"int\",\n",
    "    \"Submission Date\": \"str\"\n",
    "} # <-- configure according to your schema\n",
    "\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    for record in event[\"Records\"]:\n",
    "        bucket = record[\"s3\"][\"bucket\"][\"name\"]\n",
    "        key = record[\"s3\"][\"object\"][\"key\"]\n",
    "\n",
    "        if not key.startswith(\"raw-data/\"):\n",
    "            print(f\"Skipping file {key}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Validating file: s3://{bucket}/{key}\")\n",
    "\n",
    "        try:\n",
    "            obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "            body = obj[\"Body\"].read().decode(\"utf-8\").strip()\n",
    "            reader = csv.DictReader(io.StringIO(body))\n",
    "            headers = reader.fieldnames\n",
    "\n",
    "            if not headers:\n",
    "                print(\"âŒ No headers found.\")\n",
    "                move_file(bucket, key, \"invalid/\")\n",
    "                continue\n",
    "\n",
    "            expected_headers = list(EXPECTED_SCHEMA.keys())\n",
    "            if headers != expected_headers:\n",
    "                print(f\"âŒ Header mismatch. Found: {headers}, Expected: {expected_headers}\")\n",
    "                move_file(bucket, key, \"invalid/\")\n",
    "                continue\n",
    "\n",
    "            bad_row_count = 0\n",
    "            for row in reader:\n",
    "                for col, dtype in EXPECTED_SCHEMA.items():\n",
    "                    value = row.get(col, \"\").strip()\n",
    "\n",
    "                    if value == \"\":\n",
    "                        continue\n",
    "\n",
    "                    if dtype == \"int\":\n",
    "                        if not value.isdigit():\n",
    "                            bad_row_count += 1\n",
    "                            print(f\"Type error at row: {row}\")\n",
    "                            break\n",
    "\n",
    "\n",
    "            if bad_row_count > 0:\n",
    "                print(f\"âŒ Found {bad_row_count} rows with invalid data types.\")\n",
    "                move_file(bucket, key, \"invalid/\")\n",
    "            else:\n",
    "                print(\"âœ… Schema and data types valid.\")\n",
    "                move_file(bucket, key, \"valid/\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Exception while validating {key}: {e}\")\n",
    "            move_file(bucket, key, \"invalid/\")\n",
    "\n",
    "\n",
    "def move_file(bucket, source_key, dest_prefix):\n",
    "    filename = os.path.basename(source_key)\n",
    "    dest_key = f\"{dest_prefix}{filename}\"\n",
    "\n",
    "    s3.copy_object(Bucket=bucket, CopySource=f\"{bucket}/{source_key}\", Key=dest_key)\n",
    "    s3.delete_object(Bucket=bucket, Key=source_key)\n",
    "\n",
    "    print(f\"Moved {source_key} â†’ {dest_key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc66239-d77f-4ad5-8864-c1d8aa58daaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Transformation Logic\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r\"YOUR_FILE\")\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "df['Employee_Name'] = df['Employee_Name'].str.title().str.strip()\n",
    "df['Employee_Name'] = df['Employee_Name'].str.replace(r'^(Dr\\.|Mr\\.|Ms\\.|Mrs\\.)\\s*|,?\\s*(DVM|MD|PhD|DDS|Esq\\.|Jr\\.|Sr\\.)$', '', regex=True).str.strip()\n",
    "\n",
    "df['Job_Role'] = df['Job_Role'].str.title().str.strip()\n",
    "\n",
    "df['Email'] = df['Email'].str.replace('(?i)example', 'gmail', regex=True)\n",
    "df['Email'] = df['Email'].str.replace(r'(@)(.*)', lambda m: m.group(1) + m.group(2).lower(), regex=True)\n",
    "\n",
    "df['Phone_Number'] = '+63 ' + df['Phone_Number'].astype(str)\n",
    "df\n",
    "\n",
    "# Converted to PySpark\n",
    "\n",
    "import sys\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "from pyspark.context import SparkContext\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "# ðŸ§­ Step 1: Read valid data\n",
    "dyf = glueContext.create_dynamic_frame.from_options(\n",
    "    connection_type=\"s3\",\n",
    "    connection_options={\"paths\": [\"s3://YOUR_BUCKET_NAME/valid/\"]},\n",
    "    format=\"csv\",\n",
    "    format_options={\"withHeader\": True}\n",
    ")\n",
    "df = dyf.toDF().dropDuplicates()\n",
    "\n",
    "# ðŸ§© Step 2: Enforce consistent datatypes\n",
    "df = df.select(\n",
    "    F.col(\"Employee_ID\").cast(\"string\").alias(\"Employee_ID\"),\n",
    "    F.col(\"Employee_Name\").cast(\"string\").alias(\"Employee_Name\"),\n",
    "    F.col(\"College_Degree\").cast(\"string\").alias(\"College_Degree\"),\n",
    "    F.col(\"Department\").cast(\"string\").alias(\"Department\"),\n",
    "    F.col(\"Job_Role\").cast(\"string\").alias(\"Job_Role\"),\n",
    "    F.col(\"DMC_Campus\").cast(\"string\").alias(\"DMC_Campus\"),\n",
    "    F.col(\"Email\").cast(\"string\").alias(\"Email\"),\n",
    "    F.col(\"Phone_Number\").cast(\"string\").alias(\"Phone_Number\"),\n",
    "    F.col(\"Performance_Rating\").cast(\"int\").alias(\"Performance_Rating\"),\n",
    "    F.col(\"Submission Date\").cast(\"timestamp\").alias(\"Submission_Date\")\n",
    ")\n",
    "\n",
    "# ðŸ§¹ Step 3: Clean string columns\n",
    "df = df.withColumn(\n",
    "    \"Employee_Name\",\n",
    "    F.regexp_replace(\n",
    "        F.initcap(F.trim(F.col(\"Employee_Name\"))),\n",
    "        r'^(Dr\\.|Mr\\.|Ms\\.|Mrs\\.)\\s*|,?\\s*(DVM|MD|PhD|DDS|Esq\\.|Jr\\.|Sr\\.)$',\n",
    "        ''\n",
    "    )\n",
    ").withColumn(\n",
    "    \"Job_Role\", F.initcap(F.trim(F.col(\"Job_Role\")))\n",
    ").withColumn(\n",
    "    \"Submission_Date\", F.to_date(F.col(\"Submission_Date\"), \"yyyy-MM-dd\")\n",
    ").withColumn(\n",
    "    \"Email\", F.regexp_replace(F.col(\"Email\"), '(?i)example', 'gmail')\n",
    ").withColumn(\n",
    "    \"Email\",\n",
    "    F.concat(\n",
    "        F.regexp_extract(F.col(\"Email\"), r'^[^@]+', 0),\n",
    "        F.lit('@'),\n",
    "        F.lower(F.regexp_extract(F.col(\"Email\"), r'@(.+)', 1))\n",
    "    )\n",
    ").withColumn(\n",
    "    \"Phone_Number\", F.concat(F.lit(\"+63 \"), F.col(\"Phone_Number\").cast(\"string\"))\n",
    ").filter(\n",
    "    F.col(\"Email\").isNotNull() & F.col(\"Phone_Number\").isNotNull()\n",
    ")\n",
    "\n",
    "# âœ… Optional sanity check\n",
    "expected_columns = [\n",
    "    \"Employee_ID\", \"Employee_Name\", \"College_Degree\", \"Department\", \"Job_Role\",\n",
    "    \"DMC_Campus\", \"Email\", \"Phone_Number\", \"Performance_Rating\", \"Submission_Date\"\n",
    "]\n",
    "missing = [c for c in expected_columns if c not in df.columns]\n",
    "if missing:\n",
    "    print(f\"âš ï¸ Warning: Missing columns in input data: {missing}\")\n",
    "\n",
    "# ðŸª„ Step 4: Write cleaned data to S3 (Parquet)\n",
    "cleaned_dyf = DynamicFrame.fromDF(df, glueContext, \"cleaned_dyf\")\n",
    "sink = glueContext.getSink(\n",
    "    path=\"s3://YOUR_BUCKET_NAME/transformed/\",\n",
    "    connection_type=\"s3\",\n",
    "    updateBehavior=\"UPDATE_IN_DATABASE\",\n",
    "    compression=\"snappy\",\n",
    "    enableUpdateCatalog=True,\n",
    "    transformation_ctx=\"sink\"\n",
    ")\n",
    "sink.setCatalogInfo(catalogDatabase=\"YOUR_DATABASE_NAME\", catalogTableName=\"YOUR_CLEANED_TABLE_NAME\")\n",
    "sink.setFormat(\"glueparquet\")\n",
    "sink.writeFrame(cleaned_dyf)\n",
    "\n",
    "# ðŸ“¦ Step 5: Archive previous transformed files (to DEEP_ARCHIVE)\n",
    "s3 = boto3.client(\"s3\")\n",
    "bucket = \"YOUR_BUCKET_NAME\"\n",
    "response = s3.list_objects_v2(Bucket=bucket, Prefix=\"transformed/\")\n",
    "today = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "archive_prefix = f\"archive/{today}/\"\n",
    "\n",
    "if \"Contents\" in response:\n",
    "    for obj in response[\"Contents\"]:\n",
    "        copy_source = {\"Bucket\": bucket, \"Key\": obj[\"Key\"]}\n",
    "        archive_key = obj[\"Key\"].replace(\"transformed/\", archive_prefix)\n",
    "        s3.copy_object(\n",
    "            CopySource=copy_source,\n",
    "            Bucket=bucket,\n",
    "            Key=archive_key,\n",
    "            StorageClass=\"DEEP_ARCHIVE\"\n",
    "        )\n",
    "\n",
    "print(\"âœ… Glue transformation, type enforcement, and archiving complete.\")\n",
    "job.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd68915c-4209-4cf4-a477-0b2f9b33db40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAMBDA-ATHENA GLOBAL DEDUPLICATION\n",
    "\n",
    "import boto3\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    athena = boto3.client('athena')\n",
    "\n",
    "    query = \"\"\"\n",
    "    CREATE OR REPLACE VIEW \"YOUR_DATABASE_NAME\".\"YOUR_DEDUPLICATED_NAME\" AS\n",
    "    SELECT *\n",
    "    FROM (\n",
    "        SELECT *,\n",
    "               ROW_NUMBER() OVER (PARTITION BY employee_id) AS rn\n",
    "        FROM \"YOUR_DATABASE_NAME\".\"YOUR_CLEAN_TABLE_NAME\"\n",
    "    )\n",
    "    WHERE rn = 1;\n",
    "    \"\"\"\n",
    "\n",
    "    response = athena.start_query_execution(\n",
    "        QueryString=query,\n",
    "        QueryExecutionContext={\n",
    "            'Catalog': 'AwsDataCatalog',\n",
    "            'Database': 'YOUR_DATABASE_NAME'\n",
    "        },\n",
    "        ResultConfiguration={\n",
    "            'OutputLocation': 's3://YOUR_ATHENA_QUERY_LOCATION/'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(f\"Athena deduplication view created. Query ID: {response['QueryExecutionId']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6d2925-6c42-449d-80f3-9b153697f89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python File Exporter with Checkpoints\n",
    "\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import io\n",
    "from datetime import datetime\n",
    "\n",
    "ACCESS_KEY = \"YOUR_SECRET_KEY\"\n",
    "SECRET_KEY = \"YOUR_SECRET_KEY\"\n",
    "REGION = \"YOUR_REGION\"\n",
    "BUCKET_NAME = \"YOUR_BUCKET_NAME\"\n",
    "CHECKPOINT_KEY = \"checkpoints/last_submission_datetime.txt\"\n",
    "SHEET_URL = \"YOUR_FORM_LINK/export?format=csv\"\n",
    "\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    aws_access_key_id=ACCESS_KEY,\n",
    "    aws_secret_access_key=SECRET_KEY,\n",
    "    region_name=REGION\n",
    ")\n",
    "\n",
    "# ðŸ§­ Step 1: Load last checkpoint (datetime-aware)\n",
    "try:\n",
    "    checkpoint_obj = s3.get_object(Bucket=BUCKET_NAME, Key=CHECKPOINT_KEY)\n",
    "    last_checkpoint = checkpoint_obj[\"Body\"].read().decode(\"utf-8\").strip()\n",
    "    last_checkpoint = pd.to_datetime(last_checkpoint)\n",
    "    print(f\"ðŸ“ Last checkpoint: {last_checkpoint}\")\n",
    "except s3.exceptions.ClientError:\n",
    "    last_checkpoint = None\n",
    "    print(\"âš ï¸ No checkpoint found, exporting all data...\")\n",
    "\n",
    "# ðŸ§­ Step 2: Load sheet and parse datetime\n",
    "df = pd.read_csv(SHEET_URL)\n",
    "df[\"Submission Date\"] = pd.to_datetime(df[\"Submission Date\"], errors=\"coerce\")\n",
    "\n",
    "if \"Submission ID\" in df.columns:\n",
    "    df = df.drop(columns=[\"Submission ID\"])\n",
    "\n",
    "# ðŸ§­ Step 3: Filter new rows based on datetime\n",
    "if last_checkpoint is not None:\n",
    "    df = df[df[\"Submission Date\"] > last_checkpoint]\n",
    "\n",
    "if df.empty:\n",
    "    print(\"âœ… No new submissions since last checkpoint.\")\n",
    "else:\n",
    "    # âœ… For export readability â€” show only date in CSV\n",
    "    df_export = df.copy()\n",
    "    df_export[\"Submission Date\"] = df_export[\"Submission Date\"].dt.date\n",
    "\n",
    "    # ðŸ§­ Step 4: Export filtered data to S3\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    file_name = f\"google_sheet_responses_{timestamp}.csv\"\n",
    "    s3_key = f\"raw-data/{file_name}\"\n",
    "\n",
    "    csv_buffer = io.StringIO()\n",
    "    df_export.to_csv(csv_buffer, index=False)\n",
    "    s3.put_object(Bucket=BUCKET_NAME, Key=s3_key, Body=csv_buffer.getvalue())\n",
    "    print(f\"âœ… Uploaded new data to s3://{BUCKET_NAME}/{s3_key}\")\n",
    "\n",
    "    # ðŸ§­ Step 5: Update checkpoint with full datetime precision\n",
    "    new_checkpoint = df[\"Submission Date\"].max()\n",
    "    new_checkpoint_str = new_checkpoint.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    s3.put_object(Bucket=BUCKET_NAME, Key=CHECKPOINT_KEY, Body=new_checkpoint_str)\n",
    "    print(f\"ðŸ•’ Updated checkpoint to {new_checkpoint_str}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
